{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D, Conv2DTranspose, UpSampling2D, Reshape, GRU, concatenate, LeakyReLU, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "\n",
    "import vizdoom as vzd\n",
    "\n",
    "from random import choice\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_AU movie is made with many frames\n",
    "# Let's continue by skipping frames to have more diverse positions of the monster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'video.avi'\n",
    "vizdoom_path = \"../../../../Mech Punk/Anaconda3/envs/vizdoom/lib/vizdoom/scenarios/\"\n",
    "fps = 10\n",
    "image_counter=126742+1\n",
    "memory_size = 32000\n",
    "batch_size = 32\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_layer = Input([480, 640,3])\n",
    "\n",
    "# encoder_layers = Conv2D(8, (3,3), activation=\"relu\", padding=\"same\")(encoder_input_layer)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Conv2D(16, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Flatten()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Dense(32)(encoder_layers)\n",
    "\n",
    "# # decoder_input_layer = Input([32])\n",
    "\n",
    "# decoder_layers = Dense(38400, input_shape=[32])(encoder_layers)\n",
    "\n",
    "# decoder_layers = Reshape([30, 40, 32])(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(16, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(8, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = Conv2D(3, (3,3), activation=\"sigmoid\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# # Models \n",
    "# autoencoder_model = Model(encoder_input_layer, decoder_layers)\n",
    "# encoder_model = Model(encoder_input_layer, encoder_layers)\n",
    "# decoder_model = Sequential(autoencoder_model.layers[11:])\n",
    "\n",
    "# # Compile model\n",
    "# autoencoder_model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_model.load_weights(\"./weights/temp_{}.h5\".format(126742))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "encoder_input_layer = Input([480, 640,3])\n",
    "\n",
    "encoder_layers = Conv2D(8, (3,3), activation=\"relu\", padding=\"same\")(encoder_input_layer)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Conv2D(16, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Flatten()(encoder_layers)\n",
    "\n",
    "encoder_layers = Dense(32)(encoder_layers)\n",
    "\n",
    "\n",
    "# Create the Variational encoder part\n",
    "z_mean = Dense(16, name='z_mean')(encoder_layers)\n",
    "z_log_var = Dense(16, name='z_log_var')(encoder_layers)\n",
    "encoder_layers = Lambda(sampling, output_shape=(16,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# variational_encoder_kl_part = Model(input_layer, [z_mean, z_log_var, z])\n",
    "\n",
    "\n",
    "# decoder_input_layer = Input([32])\n",
    "\n",
    "decoder_layers = Dense(38400, input_shape=[16])(encoder_layers)\n",
    "\n",
    "decoder_layers = Reshape([30, 40, 32])(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(16, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(8, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = Conv2D(3, (3,3), activation=\"sigmoid\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# Models \n",
    "autoencoder_model = Model(encoder_input_layer, decoder_layers)\n",
    "encoder_model = Model(encoder_input_layer, encoder_layers)\n",
    "decoder_model = Sequential(autoencoder_model.layers[14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model.load_weights(\"./pretrained_weights/VAE_good_10_04_2020.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save(\"VAE_decoder_good_10_04_2020\",save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 38400)             652800    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 30, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 60, 80, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 120, 160, 32)      9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 240, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 240, 320, 16)      4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 480, 640, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 480, 640, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 480, 640, 3)       219       \n",
      "=================================================================\n",
      "Total params: 677,299\n",
      "Trainable params: 677,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: VAE_good_10_04_2020\\assets\n"
     ]
    }
   ],
   "source": [
    "# autoencoder_model.save(\"VAE_good_10_04_2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(encoded_frames_history, actions_history, steps_history, l=10):\n",
    "    steps = [1]\n",
    "    while 1 in steps:\n",
    "        index = np.random.randint(len(encoded_frames_history)-(l+1))\n",
    "        steps = steps_history[index:index+(l+1)]\n",
    "    encoded_frames = encoded_frames_history[index:index+(l+1)]\n",
    "    actions = actions_history[index:index+l]\n",
    "    return [(np.array(encoded_frames[:-1]),np.array(actions)),encoded_frames[-1]]\n",
    "\n",
    "def get_samples(encoded_frames_history, actions_history, steps_history, l=10, batch_size=32):\n",
    "    encoded_frame_samples = []\n",
    "    action_samples = []\n",
    "    prediction_samples = []\n",
    "    for i in range(batch_size):\n",
    "        sample = get_sample(encoded_frames_history, actions_history, steps_history, l)\n",
    "        encoded_frame_samples.append(sample[0][0])\n",
    "        action_samples.append(sample[0][1])\n",
    "        prediction_samples.append(sample[1])\n",
    "    return (np.array(encoded_frame_samples), np.array(action_samples)), np.array(prediction_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample = get_sample(encoded_frames_history, actions_history, steps_history, l=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = get_samples(encoded_frames_history, actions_history, steps_history, l=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_frames = 6\n",
    "\n",
    "number_of_parameters = 16\n",
    "\n",
    "frame_input = Input((number_of_frames, number_of_parameters))\n",
    "action_input = Input((number_of_frames,3))\n",
    "\n",
    "inputs = concatenate([frame_input, action_input],axis=2)\n",
    "\n",
    "# inputs = Input((number_of_frames, number_of_parameters + 3))\n",
    "\n",
    "# reset_after=True must be added due to a bug:\n",
    "# https://github.com/tensorflow/tfjs/issues/2442\n",
    "layers = GRU(32, reset_after=True)(inputs)\n",
    "\n",
    "layers = Dense(32)(layers)\n",
    "layers = LeakyReLU()(layers)\n",
    "layers = BatchNormalization()(layers)\n",
    "\n",
    "layers = Dense(32)(layers)\n",
    "layers = LeakyReLU()(layers)\n",
    "# layers = BatchNormalization()(layers)\n",
    "layers = Dense(number_of_parameters)(layers)\n",
    "\n",
    "extrapolation_model = Model((frame_input, action_input), layers)\n",
    "# extrapolation_model = Model(inputs, layers)\n",
    "\n",
    "extrapolation_model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation_model.save(\"pretrained_models/extrapolation_test.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = extrapolation_model.predict((sample[0][0].reshape(1,10,32), sample[0][1].reshape(1,10,3)))[0]\n",
    "# predictions = extrapolation_model.predict(samples[0])\n",
    "# test_loss = extrapolation_model.evaluate(samples[0], samples[1],verbose=False)\n",
    "# train_loss = extrapolation_model.train_on_batch(samples[0], samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_frames_history = []\n",
    "actions_history = []\n",
    "steps_history = []\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1:   \n",
      "\n",
      "\n",
      "Episode #2:   \n",
      "\n",
      "\n",
      "Episode #3:   \n",
      "\n",
      "\n",
      "Episode #4:   \n",
      "\n",
      "\n",
      "Episode #5:   \n",
      "\n",
      "\n",
      "Episode #6:   \n",
      "\n",
      "\n",
      "Episode #7:   \n",
      "\n",
      "\n",
      "Episode #8:   \n",
      "\n",
      "\n",
      "Episode #9:   \n",
      "\n",
      "\n",
      "Episode #10:   \n",
      "\n",
      "\n",
      "Episode #11:   \n",
      "\n",
      "\n",
      "Episode #12:   \n",
      "\n",
      "\n",
      "Episode #13:   \n",
      "\n",
      "\n",
      "Episode #14:   \n",
      "\n",
      "\n",
      "Episode #15:   \n",
      "\n",
      "\n",
      "Episode #16:   \n",
      "\n",
      "\n",
      "Episode #17:   \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create DoomGame instance. It will run the game and communicate with you.\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    # Now it's time for configuration!\n",
    "    # load_config could be used to load configuration instead of doing it here with code.\n",
    "    # If load_config is used in-code configuration will also work - most recent changes will add to previous ones.\n",
    "    # game.load_config(\"../../scenarios/basic.cfg\")\n",
    "\n",
    "    # Sets path to additional resources wad file which is basically your scenario wad.\n",
    "    # If not specified default maps will be used and it's pretty much useless... unless you want to play good old Doom.\n",
    "    game.set_doom_scenario_path(vizdoom_path + \"basic.wad\")\n",
    "    # Sets map to start (scenario .wad files can contain many maps).\n",
    "    game.set_doom_map(\"map01\")\n",
    "\n",
    "    # Sets resolution. Default is 320X240\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "\n",
    "    # Sets the screen buffer format. Not used here but now you can change it. Default is CRCGCB.\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "\n",
    "    # Enables depth buffer.\n",
    "    game.set_depth_buffer_enabled(True)\n",
    "\n",
    "    # Enables labeling of in game objects labeling.\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "\n",
    "    # Enables buffer with top down map of the current episode/level.\n",
    "    game.set_automap_buffer_enabled(True)\n",
    "\n",
    "    # Enables information about all objects present in the current episode/level.\n",
    "    game.set_objects_info_enabled(True)\n",
    "\n",
    "    # Enables information about all sectors (map layout).\n",
    "    game.set_sectors_info_enabled(True)\n",
    "\n",
    "    # Sets other rendering options (all of these options except crosshair are enabled (set to True) by default)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)  # If hud is enabled\n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)  # Bullet holes and blood on the walls\n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)  # Smoke and blood\n",
    "    game.set_render_messages(False)  # In-game messages\n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(True)  # Effect upon taking damage or picking up items\n",
    "\n",
    "    # Adds buttons that will be allowed.\n",
    "    game.add_available_button(vzd.Button.MOVE_LEFT)\n",
    "    game.add_available_button(vzd.Button.MOVE_RIGHT)\n",
    "    game.add_available_button(vzd.Button.ATTACK)\n",
    "\n",
    "    # Adds game variables that will be included in state.\n",
    "    game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "\n",
    "    # Causes episodes to finish after 200 tics (actions)\n",
    "    game.set_episode_timeout(200)\n",
    "\n",
    "    # Makes episodes start after 10 tics (~after raising the weapon)\n",
    "    game.set_episode_start_time(10)\n",
    "\n",
    "    # Makes the window appear (turned on by default)\n",
    "    game.set_window_visible(False)\n",
    "\n",
    "    # Turns on the sound. (turned off by default)\n",
    "    game.set_sound_enabled(False)\n",
    "\n",
    "    # Sets the livin reward (for each move) to -1\n",
    "    game.set_living_reward(-1)\n",
    "\n",
    "    # Sets ViZDoom mode (PLAYER, ASYNC_PLAYER, SPECTATOR, ASYNC_SPECTATOR, PLAYER mode is default)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "\n",
    "    # Enables engine output to console.\n",
    "    #game.set_console_enabled(True)\n",
    "\n",
    "    # Initialize the game. Further configuration won't take any effect from now on.\n",
    "    game.init()\n",
    "\n",
    "    # Define some actions. Each list entry corresponds to declared buttons:\n",
    "    # MOVE_LEFT, MOVE_RIGHT, ATTACK\n",
    "    # game.get_available_buttons_size() can be used to check the number of available buttons.\n",
    "    # 5 more combinations are naturally possible but only 3 are included for transparency when watching.\n",
    "    actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "\n",
    "    # Run this many episodes\n",
    "    episodes = 10000\n",
    "\n",
    "    # Sets time that will pause the engine after each action (in seconds)\n",
    "    # Without this everything would go too fast for you to keep track of what's happening.\n",
    "    sleep_time = 1.0 / vzd.DEFAULT_TICRATE  # = 0.028\n",
    "    \n",
    "    frame_counter = 0\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"Episode #{}:   \".format(i + 1))\n",
    "#         if i % 1000 == 0:\n",
    "#             autoencoder_model.save_weights(\"./weights/temp_{}.h5\".format(image_counter))\n",
    "\n",
    "        # Starts a new episode. It is not needed right after init() but it doesn't cost much. At least the loop is nicer.\n",
    "        game.new_episode()\n",
    "\n",
    "        action_counter = 0\n",
    "        step = 0\n",
    "        while not game.is_episode_finished():\n",
    "            \n",
    "            step+=1\n",
    "            \n",
    "            action_counter-=1\n",
    "            frame_counter+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Gets the state\n",
    "            state = game.get_state()\n",
    "\n",
    "            # Which consists of:\n",
    "            n = state.number\n",
    "            vars = state.game_variables\n",
    "            screen_buf = state.screen_buffer\n",
    "            depth_buf = state.depth_buffer\n",
    "            labels_buf = state.labels_buffer\n",
    "            automap_buf = state.automap_buffer\n",
    "            labels = state.labels\n",
    "            objects = state.objects\n",
    "            sectors = state.sectors\n",
    "            \n",
    "            \n",
    "            assert len(encoded_frames_history) == len(actions_history)\n",
    "            assert len(actions_history) == len(steps_history)\n",
    "            \n",
    "            frame = screen_buf/255\n",
    "            encoded_frame = encoder_model.predict(np.expand_dims(frame,0))[0]\n",
    "            encoded_frames_history.append(encoded_frame)\n",
    "            actions_history.append(action)\n",
    "            steps_history.append(step)\n",
    "            \n",
    "            if len(encoded_frames_history) > min(memory_size, 100 * batch_size):\n",
    "                samples = get_samples(encoded_frames_history, actions_history, steps_history, l=number_of_frames, batch_size=128)\n",
    "                \n",
    "                test_loss = extrapolation_model.evaluate(samples[0], samples[1],verbose=False)\n",
    "                train_loss = extrapolation_model.train_on_batch(samples[0], samples[1])\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                print(np.mean(test_losses[-10:]),end=\"\\r\")\n",
    "                \n",
    "            \n",
    "            while len(encoded_frames_history) > memory_size:\n",
    "                encoded_frames_history.pop(0)\n",
    "                actions_history.pop(0)\n",
    "                steps_history.pop(0)\n",
    "\n",
    "            if action_counter < 0:\n",
    "                action = choice(actions)\n",
    "                action_counter = np.random.randint(4,18)\n",
    "            r = game.make_action(action)\n",
    "            \n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # It will be done automatically anyway but sometimes you need to do it in the middle of the program...\n",
    "    game.close()\n",
    "#     extrapolation_model.save_weights(\"extrapolation_2_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1850388f388>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "plt.plot(train_losses)\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation_model.save_weights(\"pretrained_weights/extrapolation_VAE_good_model_weights_.h5\")\n",
    "extrapolation_model.load_weights(\"pretrained_weights/extrapolation_VAE_good_model_weights.h5\")\n",
    "\n",
    "# extrapolation_model.save_weights(\"pretrained_weights/extrapolation_AE_aweful_model_weights_.h5\")\n",
    "# extrapolation_model.load_weights(\"pretrained_weights/extrapolation_AE_aweful_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation_model.save(\"extrapolation_VAE_good_model_weights\",save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_frames_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_encoding_values = np.min(encoded_frames_history,axis=0)\n",
    "max_encoding_values = np.max(encoded_frames_history,axis=0)\n",
    "mean_encoding_values = np.mean(encoded_frames_history,axis=0)\n",
    "std_encoding_values = np.std(encoded_frames_history,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "left\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "shoot\n",
      "shoot\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "left\n",
      "shoot\n",
      "right\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "shoot\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "shoot\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "shoot\n",
      "shoot\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "left\n",
      "left\n",
      "shoot\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "shoot\n",
      "shoot\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-211a98310d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample = get_samples(encoded_frames_history, actions_history, steps_history, l=number_of_frames, batch_size=1)\n",
    "frames = sample[0][0].copy()\n",
    "actions = sample[0][1].copy()\n",
    "\n",
    "action_index = 0\n",
    "\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode([640, 480])\n",
    "action_index = 0\n",
    "while True:\n",
    "    events = pygame.event.get()\n",
    "    for event in events:\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_LEFT:\n",
    "                print(\"left\")\n",
    "                action_index = 1\n",
    "            if event.key == pygame.K_RIGHT:\n",
    "                print(\"right\")\n",
    "                action_index = 0\n",
    "            if event.key == pygame.K_UP:\n",
    "                print(\"shoot\")\n",
    "                action_index = 2\n",
    "            action = [False, False, False]\n",
    "            action[action_index]=True\n",
    "    \n",
    "\n",
    "    next_encoded_frame = extrapolation_model.predict((frames, actions))\n",
    "#     next_encoded_frame = np.clip(next_encoded_frame, mean_encoding_values-2*std_encoding_values, mean_encoding_values+2*std_encoding_values)\n",
    "    \n",
    "    frames[0] = np.concatenate((frames[0,1:],next_encoded_frame))\n",
    "    actions[0] = np.concatenate((actions[0,1:],np.array([action])))\n",
    "    \n",
    "    gridarray = np.array(255*decoder_model.predict(next_encoded_frame)[0],dtype=\"int\")\n",
    "    surface = pygame.surfarray.make_surface(gridarray)\n",
    "    surface = pygame.transform.scale(surface, (200, 200))  # Scaled a bit.\n",
    "    surface = pygame.transform.rotozoom(surface, -90, 1)\n",
    "    \n",
    "    screen.blit(surface, (0, 0))\n",
    "    pygame.display.update()\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_model.predict(next_encoded_frame)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
