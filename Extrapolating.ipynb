{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D, Conv2DTranspose, UpSampling2D, Reshape, SimpleRNN,GRU, concatenate, LeakyReLU, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "\n",
    "import vizdoom as vzd\n",
    "\n",
    "from random import choice\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_AU movie is made with many frames\n",
    "# Let's continue by skipping frames to have more diverse positions of the monster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'video.avi'\n",
    "vizdoom_path = \"../../../../Mech Punk/Anaconda3/envs/vizdoom/lib/vizdoom/scenarios/\"\n",
    "fps = 10\n",
    "image_counter=126742+1\n",
    "memory_size = 32000\n",
    "batch_size = 32\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_layer = Input([480, 640,3])\n",
    "\n",
    "# encoder_layers = Conv2D(8, (3,3), activation=\"relu\", padding=\"same\")(encoder_input_layer)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Conv2D(16, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "# encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Flatten()(encoder_layers)\n",
    "\n",
    "# encoder_layers = Dense(32)(encoder_layers)\n",
    "\n",
    "# # decoder_input_layer = Input([32])\n",
    "\n",
    "# decoder_layers = Dense(38400, input_shape=[32])(encoder_layers)\n",
    "\n",
    "# decoder_layers = Reshape([30, 40, 32])(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(16, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = UpSampling2D()(decoder_layers)\n",
    "# decoder_layers = Conv2DTranspose(8, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# decoder_layers = Conv2D(3, (3,3), activation=\"sigmoid\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# # Models \n",
    "# autoencoder_model = Model(encoder_input_layer, decoder_layers)\n",
    "# encoder_model = Model(encoder_input_layer, encoder_layers)\n",
    "# decoder_model = Sequential(autoencoder_model.layers[11:])\n",
    "\n",
    "# # Compile model\n",
    "# autoencoder_model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_model.load_weights(\"./weights/temp_{}.h5\".format(126742))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "encoder_input_layer = Input([480, 640,3])\n",
    "\n",
    "encoder_layers = Conv2D(8, (3,3), activation=\"relu\", padding=\"same\")(encoder_input_layer)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Conv2D(16, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(encoder_layers)\n",
    "encoder_layers = MaxPool2D()(encoder_layers)\n",
    "\n",
    "encoder_layers = Flatten()(encoder_layers)\n",
    "\n",
    "encoder_layers = Dense(32)(encoder_layers)\n",
    "\n",
    "\n",
    "# Create the Variational encoder part\n",
    "z_mean = Dense(16, name='z_mean')(encoder_layers)\n",
    "z_log_var = Dense(16, name='z_log_var')(encoder_layers)\n",
    "encoder_layers = Lambda(sampling, output_shape=(16,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# variational_encoder_kl_part = Model(input_layer, [z_mean, z_log_var, z])\n",
    "\n",
    "\n",
    "# decoder_input_layer = Input([32])\n",
    "\n",
    "decoder_layers = Dense(38400, input_shape=[16])(encoder_layers)\n",
    "\n",
    "decoder_layers = Reshape([30, 40, 32])(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(32, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(16, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = UpSampling2D()(decoder_layers)\n",
    "decoder_layers = Conv2DTranspose(8, (3,3), activation=\"relu\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "decoder_layers = Conv2D(3, (3,3), activation=\"sigmoid\", padding=\"same\")(decoder_layers)\n",
    "\n",
    "# Models \n",
    "autoencoder_model = Model(encoder_input_layer, decoder_layers)\n",
    "encoder_model = Model(encoder_input_layer, encoder_layers)\n",
    "decoder_model = Sequential(autoencoder_model.layers[14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model.load_weights(\"./pretrained_weights/VAE_good_10_04_2020.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save(\"VAE_decoder_good_10_04_2020\",save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 38400)             652800    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 30, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 60, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 60, 80, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 120, 160, 32)      9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 240, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 240, 320, 16)      4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 480, 640, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 480, 640, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 480, 640, 3)       219       \n",
      "=================================================================\n",
      "Total params: 677,299\n",
      "Trainable params: 677,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: VAE_good_10_04_2020\\assets\n"
     ]
    }
   ],
   "source": [
    "# autoencoder_model.save(\"VAE_good_10_04_2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(encoded_frames_history, actions_history, steps_history, l=10):\n",
    "    steps = [1]\n",
    "    while 1 in steps:\n",
    "        index = np.random.randint(len(encoded_frames_history)-(l+1))\n",
    "        steps = steps_history[index:index+(l+1)]\n",
    "    encoded_frames = encoded_frames_history[index:index+(l+1)]\n",
    "    actions = actions_history[index:index+l]\n",
    "    return [(np.array(encoded_frames[:-1]),np.array(actions)),encoded_frames[-1]]\n",
    "\n",
    "def get_samples(encoded_frames_history, actions_history, steps_history, l=10, batch_size=32):\n",
    "    encoded_frame_samples = []\n",
    "    action_samples = []\n",
    "    prediction_samples = []\n",
    "    for i in range(batch_size):\n",
    "        sample = get_sample(encoded_frames_history, actions_history, steps_history, l)\n",
    "        encoded_frame_samples.append(sample[0][0])\n",
    "        action_samples.append(sample[0][1])\n",
    "        prediction_samples.append(sample[1])\n",
    "    return (np.array(encoded_frame_samples), np.array(action_samples)), np.array(prediction_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample = get_sample(encoded_frames_history, actions_history, steps_history, l=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = get_samples(encoded_frames_history, actions_history, steps_history, l=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_frames = 6\n",
    "\n",
    "number_of_parameters = 16\n",
    "\n",
    "frame_input = Input((number_of_frames, number_of_parameters))\n",
    "action_input = Input((number_of_frames,3))\n",
    "\n",
    "inputs = concatenate([frame_input, action_input],axis=2)\n",
    "\n",
    "# inputs = Input((number_of_frames, number_of_parameters + 3))\n",
    "\n",
    "# reset_after=True must be added due to a bug:\n",
    "# https://github.com/tensorflow/tfjs/issues/2442\n",
    "# However! stil no success\n",
    "# layers = GRU(32, reset_after=True)(inputs)\n",
    "\n",
    "layers = SimpleRNN(32)(inputs)\n",
    "\n",
    "layers = Dense(32)(layers)\n",
    "layers = LeakyReLU()(layers)\n",
    "layers = BatchNormalization()(layers)\n",
    "\n",
    "layers = Dense(32)(layers)\n",
    "layers = LeakyReLU()(layers)\n",
    "# layers = BatchNormalization()(layers)\n",
    "layers = Dense(number_of_parameters)(layers)\n",
    "\n",
    "extrapolation_model = Model((frame_input, action_input), layers)\n",
    "# extrapolation_model = Model(inputs, layers)\n",
    "\n",
    "extrapolation_model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation_model.save(\"pretrained_models/extrapolation_simple_.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = extrapolation_model.predict((sample[0][0].reshape(1,10,32), sample[0][1].reshape(1,10,3)))[0]\n",
    "# predictions = extrapolation_model.predict(samples[0])\n",
    "# test_loss = extrapolation_model.evaluate(samples[0], samples[1],verbose=False)\n",
    "# train_loss = extrapolation_model.train_on_batch(samples[0], samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_frames_history = []\n",
    "actions_history = []\n",
    "steps_history = []\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1:   \n",
      "0.6767985656857497\n",
      "\n",
      "Episode #2:   \n",
      "0.6749998569488526\n",
      "\n",
      "Episode #3:   \n",
      "0.6709678664803504\n",
      "\n",
      "Episode #4:   \n",
      "0.6761023789644242\n",
      "\n",
      "Episode #5:   \n",
      "0.6646459817886352\n",
      "\n",
      "Episode #6:   \n",
      "0.6731902584433556\n",
      "\n",
      "Episode #7:   \n",
      "0.6676229491829873\n",
      "\n",
      "Episode #8:   \n",
      "0.6551325052976609\n",
      "\n",
      "Episode #9:   \n",
      "0.6641586616635322\n",
      "\n",
      "Episode #10:   \n",
      "0.6571631908416748\n",
      "\n",
      "Episode #11:   \n",
      "0.6527461692690849\n",
      "\n",
      "Episode #12:   \n",
      "0.6774027764797215\n",
      "\n",
      "Episode #13:   \n",
      "0.6597178399562835\n",
      "\n",
      "Episode #14:   \n",
      "0.6661540448665619\n",
      "\n",
      "Episode #15:   \n",
      "0.6571366876363754\n",
      "\n",
      "Episode #16:   \n",
      "0.6632355734705925\n",
      "\n",
      "Episode #17:   \n",
      "0.6633279845118523\n",
      "\n",
      "Episode #18:   \n",
      "0.6716410025954247\n",
      "\n",
      "Episode #19:   \n",
      "0.6665563285350847\n",
      "\n",
      "Episode #20:   \n",
      "0.6593293577432633\n",
      "\n",
      "Episode #21:   \n",
      "0.6498646125197413\n",
      "\n",
      "Episode #22:   \n",
      "0.6577954977750778\n",
      "\n",
      "Episode #23:   \n",
      "0.6690633982419968\n",
      "\n",
      "Episode #24:   \n",
      "0.6706193536520004\n",
      "\n",
      "Episode #25:   \n",
      "0.6549000963568687\n",
      "\n",
      "Episode #26:   \n",
      "0.6656528264284134\n",
      "\n",
      "Episode #27:   \n",
      "0.6635615944862365\n",
      "\n",
      "Episode #28:   \n",
      "0.6446551620960236\n",
      "\n",
      "Episode #29:   \n",
      "0.6637030869722367\n",
      "\n",
      "Episode #30:   \n",
      "0.6530102983117103\n",
      "\n",
      "Episode #31:   \n",
      "0.6421781405806541\n",
      "\n",
      "Episode #32:   \n",
      "0.6542732745409012\n",
      "\n",
      "Episode #33:   \n",
      "0.6542747840285301\n",
      "\n",
      "Episode #34:   \n",
      "0.6523098319768905\n",
      "\n",
      "Episode #35:   \n",
      "0.6358113184571266\n",
      "\n",
      "Episode #36:   \n",
      "0.6574244946241379\n",
      "\n",
      "Episode #37:   \n",
      "0.6503219529986382\n",
      "\n",
      "Episode #38:   \n",
      "0.6500734165310862\n",
      "\n",
      "Episode #39:   \n",
      "0.6475111633539239\n",
      "\n",
      "Episode #40:   \n",
      "0.6542852401733399\n",
      "\n",
      "Episode #41:   \n",
      "0.6537655264139175\n",
      "\n",
      "Episode #42:   \n",
      "0.6402072563767434\n",
      "\n",
      "Episode #43:   \n",
      "0.6456209138035774\n",
      "\n",
      "Episode #44:   \n",
      "0.6518634214997292\n",
      "\n",
      "Episode #45:   \n",
      "0.6546485096216201\n",
      "\n",
      "Episode #46:   \n",
      "0.6313125684857368\n",
      "\n",
      "Episode #47:   \n",
      "0.6471439599990845\n",
      "\n",
      "Episode #48:   \n",
      "0.6513630092144013\n",
      "\n",
      "Episode #49:   \n",
      "0.6510822489857674\n",
      "\n",
      "Episode #50:   \n",
      "0.6434683799743652\n",
      "\n",
      "Episode #51:   \n",
      "0.6557345345616341\n",
      "\n",
      "Episode #52:   \n",
      "0.6430911049246788\n",
      "\n",
      "Episode #53:   \n",
      "0.6562893748283386\n",
      "\n",
      "Episode #54:   \n",
      "0.6512679338455201\n",
      "\n",
      "Episode #55:   \n",
      "0.6388775646686554\n",
      "\n",
      "Episode #56:   \n",
      "0.6509111285209656\n",
      "\n",
      "Episode #57:   \n",
      "0.6525819510221481\n",
      "\n",
      "Episode #58:   \n",
      "0.6453113839030266\n",
      "\n",
      "Episode #59:   \n",
      "0.6439941525459299\n",
      "\n",
      "Episode #60:   \n",
      "0.6421288684010505\n",
      "\n",
      "Episode #61:   \n",
      "0.6506895720958712\n",
      "\n",
      "Episode #62:   \n",
      "0.6420347332954407\n",
      "\n",
      "Episode #63:   \n",
      "0.6481774687767029\n",
      "\n",
      "Episode #64:   \n",
      "0.6458138123154648\n",
      "\n",
      "Episode #65:   \n",
      "0.6504385158419609\n",
      "\n",
      "Episode #66:   \n",
      "0.6517236143350601\n",
      "\n",
      "Episode #67:   \n",
      "0.6422025352716446\n",
      "\n",
      "Episode #68:   \n",
      "0.6516880437731742\n",
      "\n",
      "Episode #69:   \n",
      "0.6574837014079093\n",
      "\n",
      "Episode #70:   \n",
      "0.6344643354415893\n",
      "\n",
      "Episode #71:   \n",
      "0.6426185354590416\n",
      "\n",
      "Episode #72:   \n",
      "0.6411940976977348\n",
      "\n",
      "Episode #73:   \n",
      "0.6557272195816044\n",
      "\n",
      "Episode #74:   \n",
      "0.6440882667899132\n",
      "\n",
      "Episode #75:   \n",
      "0.6515778169035912\n",
      "\n",
      "Episode #76:   \n",
      "0.6476104736328125\n",
      "\n",
      "Episode #77:   \n",
      "0.6404892981052399\n",
      "\n",
      "Episode #78:   \n",
      "0.6364043518900871\n",
      "\n",
      "Episode #79:   \n",
      "0.6326219305396086\n",
      "\n",
      "Episode #80:   \n",
      "0.6340404108166695\n",
      "\n",
      "Episode #81:   \n",
      "0.6355405449867249\n",
      "\n",
      "Episode #82:   \n",
      "0.6502900943160057\n",
      "\n",
      "Episode #83:   \n",
      "0.6460693866014481\n",
      "\n",
      "Episode #84:   \n",
      "0.6404633417725563\n",
      "\n",
      "Episode #85:   \n",
      "0.6421519801020622\n",
      "\n",
      "Episode #86:   \n",
      "0.6371238559484482\n",
      "\n",
      "Episode #87:   \n",
      "0.6358502611517907\n",
      "\n",
      "Episode #88:   \n",
      "0.6486393645405769\n",
      "\n",
      "Episode #89:   \n",
      "0.6424603983759884\n",
      "\n",
      "Episode #90:   \n",
      "0.6359552264213562\n",
      "\n",
      "Episode #91:   \n",
      "0.6380405098199844\n",
      "\n",
      "Episode #92:   \n",
      "0.6406118527054787\n",
      "\n",
      "Episode #93:   \n",
      "0.6391462177038193\n",
      "\n",
      "Episode #94:   \n",
      "0.6511636570096015\n",
      "\n",
      "Episode #95:   \n",
      "0.6338485196232796\n",
      "\n",
      "Episode #96:   \n",
      "0.6428932473063469\n",
      "\n",
      "Episode #97:   \n",
      "0.6221868649125133\n",
      "\n",
      "Episode #98:   \n",
      "0.6378509119153023\n",
      "\n",
      "Episode #99:   \n",
      "0.6368317320942879\n",
      "\n",
      "Episode #100:   \n",
      "0.6362573206424713\n",
      "\n",
      "Episode #101:   \n",
      "0.6384296059608461\n",
      "\n",
      "Episode #102:   \n",
      "0.6332626998424536\n",
      "\n",
      "Episode #103:   \n",
      "0.6380755230784416\n",
      "\n",
      "Episode #104:   \n",
      "0.6324227035045624\n",
      "\n",
      "Episode #105:   \n",
      "0.6363795384764671\n",
      "\n",
      "Episode #106:   \n",
      "0.6361658915877342\n",
      "\n",
      "Episode #107:   \n",
      "0.6379753991961479\n",
      "\n",
      "Episode #108:   \n",
      "0.6408017545938491\n",
      "\n",
      "Episode #109:   \n",
      "0.6233527250587948\n",
      "\n",
      "Episode #110:   \n",
      "0.6384471267461777\n",
      "\n",
      "Episode #111:   \n",
      "0.6201159343123436\n",
      "\n",
      "Episode #112:   \n",
      "0.6491917237639427\n",
      "\n",
      "Episode #113:   \n",
      "0.6428108245134354\n",
      "\n",
      "Episode #114:   \n",
      "0.6387420088052752\n",
      "\n",
      "Episode #115:   \n",
      "0.6363251894712448\n",
      "\n",
      "Episode #116:   \n",
      "0.6327303096652033\n",
      "\n",
      "Episode #117:   \n",
      "0.6272613123059273\n",
      "\n",
      "Episode #118:   \n",
      "0.6242943048477173\n",
      "\n",
      "Episode #119:   \n",
      "0.6374126955866813\n",
      "\n",
      "Episode #120:   \n",
      "0.6346339002251625\n",
      "\n",
      "Episode #121:   \n",
      "0.6466329470276833\n",
      "\n",
      "Episode #122:   \n",
      "0.6347799941897392\n",
      "\n",
      "Episode #123:   \n",
      "0.6222704052925118\n",
      "\n",
      "Episode #124:   \n",
      "0.6390615209937096\n",
      "\n",
      "Episode #125:   \n",
      "0.6301317453384434\n",
      "\n",
      "Episode #126:   \n",
      "0.6363118156790734\n",
      "\n",
      "Episode #127:   \n",
      "0.6348204806447029\n",
      "\n",
      "Episode #128:   \n",
      "0.6366873666644096\n",
      "\n",
      "Episode #129:   \n",
      "0.6414199903607368\n",
      "\n",
      "Episode #130:   \n",
      "0.6426917031407356\n",
      "\n",
      "Episode #131:   \n",
      "0.6318861693143845\n",
      "\n",
      "Episode #132:   \n",
      "0.6369060337543487\n",
      "\n",
      "Episode #133:   \n",
      "0.6279609471559524\n",
      "\n",
      "Episode #134:   \n",
      "0.6324579626321792\n",
      "\n",
      "Episode #135:   \n",
      "0.6227858796715736\n",
      "\n",
      "Episode #136:   \n",
      "0.6282925173640251\n",
      "\n",
      "Episode #137:   \n",
      "0.6311438545584679\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-37510b5cca43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextrapolation_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextrapolation_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[0mtest_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           standalone=True)\n\u001b[0m\u001b[0;32m   1079\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m   1080\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[0;32m    431\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vizdoom\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create DoomGame instance. It will run the game and communicate with you.\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    # Now it's time for configuration!\n",
    "    # load_config could be used to load configuration instead of doing it here with code.\n",
    "    # If load_config is used in-code configuration will also work - most recent changes will add to previous ones.\n",
    "    # game.load_config(\"../../scenarios/basic.cfg\")\n",
    "\n",
    "    # Sets path to additional resources wad file which is basically your scenario wad.\n",
    "    # If not specified default maps will be used and it's pretty much useless... unless you want to play good old Doom.\n",
    "    game.set_doom_scenario_path(vizdoom_path + \"basic.wad\")\n",
    "    # Sets map to start (scenario .wad files can contain many maps).\n",
    "    game.set_doom_map(\"map01\")\n",
    "\n",
    "    # Sets resolution. Default is 320X240\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "\n",
    "    # Sets the screen buffer format. Not used here but now you can change it. Default is CRCGCB.\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "\n",
    "    # Enables depth buffer.\n",
    "    game.set_depth_buffer_enabled(True)\n",
    "\n",
    "    # Enables labeling of in game objects labeling.\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "\n",
    "    # Enables buffer with top down map of the current episode/level.\n",
    "    game.set_automap_buffer_enabled(True)\n",
    "\n",
    "    # Enables information about all objects present in the current episode/level.\n",
    "    game.set_objects_info_enabled(True)\n",
    "\n",
    "    # Enables information about all sectors (map layout).\n",
    "    game.set_sectors_info_enabled(True)\n",
    "\n",
    "    # Sets other rendering options (all of these options except crosshair are enabled (set to True) by default)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)  # If hud is enabled\n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)  # Bullet holes and blood on the walls\n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)  # Smoke and blood\n",
    "    game.set_render_messages(False)  # In-game messages\n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(True)  # Effect upon taking damage or picking up items\n",
    "\n",
    "    # Adds buttons that will be allowed.\n",
    "    game.add_available_button(vzd.Button.MOVE_LEFT)\n",
    "    game.add_available_button(vzd.Button.MOVE_RIGHT)\n",
    "    game.add_available_button(vzd.Button.ATTACK)\n",
    "\n",
    "    # Adds game variables that will be included in state.\n",
    "    game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "\n",
    "    # Causes episodes to finish after 200 tics (actions)\n",
    "    game.set_episode_timeout(200)\n",
    "\n",
    "    # Makes episodes start after 10 tics (~after raising the weapon)\n",
    "    game.set_episode_start_time(10)\n",
    "\n",
    "    # Makes the window appear (turned on by default)\n",
    "    game.set_window_visible(False)\n",
    "\n",
    "    # Turns on the sound. (turned off by default)\n",
    "    game.set_sound_enabled(False)\n",
    "\n",
    "    # Sets the livin reward (for each move) to -1\n",
    "    game.set_living_reward(-1)\n",
    "\n",
    "    # Sets ViZDoom mode (PLAYER, ASYNC_PLAYER, SPECTATOR, ASYNC_SPECTATOR, PLAYER mode is default)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "\n",
    "    # Enables engine output to console.\n",
    "    #game.set_console_enabled(True)\n",
    "\n",
    "    # Initialize the game. Further configuration won't take any effect from now on.\n",
    "    game.init()\n",
    "\n",
    "    # Define some actions. Each list entry corresponds to declared buttons:\n",
    "    # MOVE_LEFT, MOVE_RIGHT, ATTACK\n",
    "    # game.get_available_buttons_size() can be used to check the number of available buttons.\n",
    "    # 5 more combinations are naturally possible but only 3 are included for transparency when watching.\n",
    "    actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "\n",
    "    # Run this many episodes\n",
    "    episodes = 10000\n",
    "\n",
    "    # Sets time that will pause the engine after each action (in seconds)\n",
    "    # Without this everything would go too fast for you to keep track of what's happening.\n",
    "    sleep_time = 1.0 / vzd.DEFAULT_TICRATE  # = 0.028\n",
    "    \n",
    "    frame_counter = 0\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"Episode #{}:   \".format(i + 1))\n",
    "#         if i % 1000 == 0:\n",
    "#             autoencoder_model.save_weights(\"./weights/temp_{}.h5\".format(image_counter))\n",
    "\n",
    "        # Starts a new episode. It is not needed right after init() but it doesn't cost much. At least the loop is nicer.\n",
    "        game.new_episode()\n",
    "\n",
    "        action_counter = 0\n",
    "        step = 0\n",
    "        while not game.is_episode_finished():\n",
    "            \n",
    "            step+=1\n",
    "            \n",
    "            action_counter-=1\n",
    "            frame_counter+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Gets the state\n",
    "            state = game.get_state()\n",
    "\n",
    "            # Which consists of:\n",
    "            n = state.number\n",
    "            vars = state.game_variables\n",
    "            screen_buf = state.screen_buffer\n",
    "            depth_buf = state.depth_buffer\n",
    "            labels_buf = state.labels_buffer\n",
    "            automap_buf = state.automap_buffer\n",
    "            labels = state.labels\n",
    "            objects = state.objects\n",
    "            sectors = state.sectors\n",
    "            \n",
    "            \n",
    "            assert len(encoded_frames_history) == len(actions_history)\n",
    "            assert len(actions_history) == len(steps_history)\n",
    "            \n",
    "            frame = screen_buf/255\n",
    "            encoded_frame = encoder_model.predict(np.expand_dims(frame,0))[0]\n",
    "            encoded_frames_history.append(encoded_frame)\n",
    "            actions_history.append(action)\n",
    "            steps_history.append(step)\n",
    "            \n",
    "            if len(encoded_frames_history) > min(memory_size, 100 * batch_size):\n",
    "                samples = get_samples(encoded_frames_history, actions_history, steps_history, l=number_of_frames, batch_size=128)\n",
    "                \n",
    "                test_loss = extrapolation_model.evaluate(samples[0], samples[1],verbose=False)\n",
    "                train_loss = extrapolation_model.train_on_batch(samples[0], samples[1])\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                print(np.mean(test_losses[-10:]),end=\"\\r\")\n",
    "                \n",
    "            \n",
    "            while len(encoded_frames_history) > memory_size:\n",
    "                encoded_frames_history.pop(0)\n",
    "                actions_history.pop(0)\n",
    "                steps_history.pop(0)\n",
    "\n",
    "            if action_counter < 0:\n",
    "                action = choice(actions)\n",
    "                action_counter = np.random.randint(4,18)\n",
    "            r = game.make_action(action)\n",
    "            \n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # It will be done automatically anyway but sometimes you need to do it in the middle of the program...\n",
    "    game.close()\n",
    "#     extrapolation_model.save_weights(\"extrapolation_2_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16d53cc7308>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "plt.plot(train_losses)\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation_model.save_weights(\"pretrained_weights/extrapolation_VAE_good_model_weights_.h5\")\n",
    "# extrapolation_model.load_weights(\"pretrained_weights/extrapolation_VAE_good_model_weights.h5\")\n",
    "\n",
    "# extrapolation_model.save_weights(\"pretrained_weights/extrapolation_AE_aweful_model_weights_.h5\")\n",
    "# extrapolation_model.load_weights(\"pretrained_weights/extrapolation_AE_aweful_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation_model.save(\"extrapolation_VAE_good_model_weights\",save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_frames_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_encoding_values = np.min(encoded_frames_history,axis=0)\n",
    "max_encoding_values = np.max(encoded_frames_history,axis=0)\n",
    "mean_encoding_values = np.mean(encoded_frames_history,axis=0)\n",
    "std_encoding_values = np.std(encoded_frames_history,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "left\n",
      "shoot\n",
      "shoot\n",
      "shoot\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-211a98310d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample = get_samples(encoded_frames_history, actions_history, steps_history, l=number_of_frames, batch_size=1)\n",
    "frames = sample[0][0].copy()\n",
    "actions = sample[0][1].copy()\n",
    "\n",
    "action_index = 0\n",
    "\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode([640, 480])\n",
    "action_index = 0\n",
    "while True:\n",
    "    events = pygame.event.get()\n",
    "    for event in events:\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_LEFT:\n",
    "                print(\"left\")\n",
    "                action_index = 1\n",
    "            if event.key == pygame.K_RIGHT:\n",
    "                print(\"right\")\n",
    "                action_index = 0\n",
    "            if event.key == pygame.K_UP:\n",
    "                print(\"shoot\")\n",
    "                action_index = 2\n",
    "            action = [False, False, False]\n",
    "            action[action_index]=True\n",
    "    \n",
    "\n",
    "    next_encoded_frame = extrapolation_model.predict((frames, actions))\n",
    "#     next_encoded_frame = np.clip(next_encoded_frame, mean_encoding_values-2*std_encoding_values, mean_encoding_values+2*std_encoding_values)\n",
    "    \n",
    "    frames[0] = np.concatenate((frames[0,1:],next_encoded_frame))\n",
    "    actions[0] = np.concatenate((actions[0,1:],np.array([action])))\n",
    "    \n",
    "    gridarray = np.array(255*decoder_model.predict(next_encoded_frame)[0],dtype=\"int\")\n",
    "    surface = pygame.surfarray.make_surface(gridarray)\n",
    "    surface = pygame.transform.scale(surface, (200, 200))  # Scaled a bit.\n",
    "    surface = pygame.transform.rotozoom(surface, -90, 1)\n",
    "    \n",
    "    screen.blit(surface, (0, 0))\n",
    "    pygame.display.update()\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_model.predict(next_encoded_frame)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
